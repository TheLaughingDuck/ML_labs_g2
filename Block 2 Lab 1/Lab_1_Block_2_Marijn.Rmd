---
title: "Lab_1_Block_2_Marijn"
author: "Marijn Jaarsma"
date: "2023-11-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries echo=FALSE, warning=FALSE, message=FALSE}
```

# 2. Mixture Models

```{r 2}
set.seed(1234567890)

max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data

true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}

M=3 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu

for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  
  # E-step: Computation of the weights
  # Compute x given y=m probabilities
  bern <- matrix(nrow=n, ncol=M)
  for (i in 1:n) {
    for (m in 1:M) {
      v_bern <- vector()
      for (d in 1:D) {
          bern_im <- mu[m, d] ^ x[i, d] * (1 - mu[m, d]) ^ (1 - x[i, d])  
          v_bern <- append(v_bern, bern_im)
      }
      bern[i, m] <- pi[m] * prod(v_bern)
    }
  }
  
  # Compute weights according to P(y=m|x) = P(x|y=m)P(y=m) / P(x)
  for (i in 1:n) {
    for (m in 1:M) {
      w[i, m] <- bern[i, m] * pi[m] / sum(bern[i]) 
    }
  }
  
  # Compute new pi and mu from weights
  pi <- 1 / n * colSums(w)
  
  mu[1,] <- 1 / sum(w[, 1]) * colSums(w[, 1] * x)
  mu[2,] <- 1 / sum(w[, 2]) * colSums(w[, 2] * x)
  mu[3,] <- 1 / sum(w[, 3]) * colSums(w[, 3] * x)
  
  #Log likelihood computation.
  
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the log likelihood has not changed significantly
  # Your code here
  
  #M-step: ML parameter estimation from the data and weights
  # Your code here
  
}

pi
mu
plot(llik[1:it], type="o")
```
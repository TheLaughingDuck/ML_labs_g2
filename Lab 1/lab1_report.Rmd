---
title: "Lab 1 Report"
author: "Marijn Jaarsma & Simon Jorstedt & Hugo Morvan"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r libraries, echo=FALSE}
library(caret)
library(kknn)
library(ggplot2)

```

# Assignment 1
## 1.1
> Import the data into R and divide it into training, validation and test sets (50%/25%/25%) by using the partitioning principle specified in the lecture slides. 

```{r 1.1}
# Read data
digit_data <- read.csv("optdigits.csv", header=FALSE)

# Partition data (according to Oleg)
n = dim(digit_data)[1]

set.seed(12345)
id = sample(1:n, floor(n*0.5))
digits_train = digit_data[id,]
id1 = setdiff(1:n, id)

set.seed(12345)
id2 = sample(id1, floor(n*0.25))
digits_valid = digit_data[id2,]
id3 = setdiff(id1,id2)
digits_test = digit_data[id3,]
```

## 2. 
>Use training data to fit 30-nearest neighbor classifier with function kknn() and kernel=â€rectangularâ€ from package kknn and estimate:
â€¢ Confusion matrices for the training and test data (use table())
â€¢ Misclassification errors for the training and test data

```{r 1.2}
#Fitting the classifier
digits_kknn_test <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_valid, k = 30, kernel = "rectangular")
kknn_fit_test <- fitted(digits_kknn_test)
conf_mat_test <- table(obs = digits_valid$V65, pred = kknn_fit_test)
conf_mat_test

digits_kknn_train <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_train, k = 30, kernel = "rectangular")
kknn_fit_train <- fitted(digits_kknn_train)
conf_mat_train <- table(obs = digits_train$V65, pred = kknn_fit_train)
conf_mat_train

#Misclassification error
get_mean_misc_err <- function(conf_mat){
  #Given a confusion matrix, return the misclassification error
  (sum(conf_mat)-sum(diag(conf_mat)))/sum(conf_mat)
}
get_indiv_misc_err <- function(conf_mat){
  #Given a confusion matrix, return the misclassification error for each digit
  err = c()
  for(i in 1:10){
    #correct this
    err[i] <- (sum(conf_mat[i,]-conf_mat[i,i])/sum(conf_mat[i,]))
  }
  return(Misc_error = err)
}
print("Misclassification error for test data :")
get_mean_misc_err(conf_mat_test)
get_indiv_misc_err(conf_mat_test)
print("Misclassification error for train data :")
get_mean_misc_err(conf_mat_train)
get_indiv_misc_err(conf_mat_train)
```

>Comment on the quality of predictions for different digits and on the overall
prediction quality.

Some digits have a really high quality of prediction (6 -> 0% error, 7 -> 0.9% error while others have a relatively worst quality (8 -> 10.3% error, 4 -> 13.8% error). Overall, the quality is quite good, with a mean misclassification error of 5.3% for the test data and 4.5% for the train data.

## 3.
>Find any 2 cases of digit â€œ8â€ in the training data which were easiest to classify and 3 cases that were hardest to classify (i.e. having highest and lowest probabilities of the correct class). 

```{r 1.3}
#Find all fitted values classified as 8 (CL = 8), find the max and min probabilities in prob

#Fitted values
fitted_eight <- predict(digits_kknn_train, data = digits_train)
#True values
actual_eight <- digits_train$V65
#Probabilities
probs <- digits_kknn_train[["prob"]]
#Probabilities for 8
probs_eight <- probs[,9]

my_df <- data.frame("actual" = actual_eight, "fitted" = fitted_eight , "prob" = probs_eight)
#filter my_df to only keep the 8s correctly fitted
my_df <- my_df[my_df$actual == 8,]
my_df <- my_df[my_df$fitted == 8,]

#Find the max and min probabilities
maxp <- 0
minp <- 1
for(i in 1:length(probs_eight)){
  if(factor(digits_kknn_train$fitted.values[i]) == 8){         #https://www.tutorialspoint.com/how-to-extract-the-factor-levels-from-factor-column-in-an-r-data-frame#:~:text=To%20extract%20the%20factor%20levels%20from%20factor%20column%2C%20we%20can,levels(df%24x).
    maxp <- max(probs_eight[i], maxp)
    minp <- min(probs_eight[i], minp)
  }
}
# Find all the index where p=maxp and p= minp :
high_idx = c()
low_idx = c()
for(i in 1:length(probs_eight)){
  if(probs_eight[i] == maxp){
    high_idx <- append(high_idx, i)
  }
  if(probs_eight[i] == minp){
    low_idx <- append(low_idx, i)
  }
}
high_idx
# 129  195  211  233  292  294  515  601  650  679  684  693  726  729  752  763  768  779 855  864  899  929 1006 1092 1134 1216 1227 1261 1295 1318 1355 1380 1387 1397 1419 1472 1533 1607 1646 1686
low_idx
# 141  258  469  560  629  881 1274 1716
```
> Reshape features for each of these cases as matrix 8x8 and visualize the corresponding digits (by using e.g. heatmap() function with parameters Colv=NA and Rowv=NA) and comment on whether these cases seem to be hard or easy to recognize visually.

```{r 1.3.2}
visualise_dig <- function(idx, data){
  #Given an index (idx) and a dataframe (data), visualize the digit at data[idx]
  raw_dig <- data[idx,][-65]
  mat = matrix(as.numeric(raw_dig), nrow = 8)
  heatmap(apply(t(mat),2,rev), Colv=NA, Rowv=NA, col=paste("gray",99:1,sep=""))
}
```

```{r 1.3.3}
#High probabilities
for(i in 1:2){
  visualise_dig(high_idx[i], digits_train)
}
#Low probabilities
for(i in 1:3){
  visualise_dig(low_idx[i], digits_train)
}
```

## 4.
>Fit a K-nearest neighbor classifiers to the training data for different values of K K = 1,2, â€¦ , 30 and plot the dependence of the training and validation misclassification errors on the value of K (in the same plot). 
How does the model complexity change when K increases and how does it affect the training and validation errors? Report the optimal K K according to this plot. Finally, estimate the test error for the model having the optimal K, compare it with the training and validation errors and make necessary conclusions about the model quality.

```{r 1.4}
k_values <- c()
misc_errs_test <- c()
misc_errs_train <- c()

for(i in c(1:30)){
  #Fitting the classifier
  #Test:
  digits_kknn_test_i <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_valid, k = i, kernel = "rectangular")
  #Train:
  digits_kknn_train_i <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_train, k = i, kernel = "rectangular")
  
  #kbetter <- train.kknn(formula = as.factor(V65) ~ . , data = digits_train, ks=i, kernel = "rectangular")
  #Predicting the values
  fitted_values_valid <- predict(digits_kknn_test_i, data = digits_valid)
  fitted_values_train <- predict(digits_kknn_train_i, data = digits_train)
  #Confusion matrix
  conf_mat_test <- table(obs = digits_valid$V65, pred = fitted_values_valid)
  conf_mat_train <- table(obs = digits_train$V65, pred = fitted_values_train)
  #Misclassification error
  misc_err_test <- get_mean_misc_err(conf_mat_test)
  misc_err_train <- get_mean_misc_err(conf_mat_train)
  
  #Append to the vectors
  k_values <- append(k_values, i)
  misc_errs_test <- append(misc_errs_test, misc_err_test)
  misc_errs_train <- append(misc_errs_train, misc_err_train)
}
df_ks <- data.frame("k" = k_values, "misc_err_test" = misc_errs_test, "misc_err_train" = misc_errs_train)
```

```{r 1.4 plot}
#Plotting misc_errs_test and misc_errs_train as a function of k
ggplot(df_ks, aes(x = k, y = misc_err_test)) + geom_line() + geom_point() + geom_line(aes(y = misc_err_train), color = "red") + geom_point(aes(y = misc_err_train), color = "red") + ggtitle("Misc error as a function of k") + xlab("k") + ylab("Misc error")
```


It would seems like k=3 and k=4 have the lowest valid misclassification rate.
Since k=3 have a lower training misclassification rate, we choose k=3 as the optimal k.
The model complexity increases with k but it does not seem to affect the training and validation errors after k=3.

```{r 1.4.2}
#Estimation of the model for k=3

#Fitting the classifier
digits_kknn_test_3 <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_test, k = 3, kernel = "rectangular")
digits_kknn_train_3 <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_train, k = 3, kernel = "rectangular")
digits_kknn_valid_3 <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_valid, k = 3, kernel = "rectangular")

#Predicting the values
fitted_values_test_3 <- predict(digits_kknn_test_3, data = digits_test)
fitted_values_train_3 <- predict(digits_kknn_train_3, data = digits_train)
fitted_values_valid_3 <- predict(digits_kknn_valid_3, data = digits_valid)
#Validation error
conf_mat_test_3 <- table(obs = digits_test$V65, pred = fitted_values_test_3)
conf_mat_train_3 <- table(obs = digits_train$V65, pred = fitted_values_train_3)
conf_mat_valid_3 <- table(obs = digits_valid$V65, pred = fitted_values_valid_3)
#Misclassification error
misc_err_test_3 <- get_mean_misc_err(conf_mat_test_3)
misc_err_train_3 <- get_mean_misc_err(conf_mat_train_3)
misc_err_valid_3 <- get_mean_misc_err(conf_mat_valid_3)

print(paste("Test error for k=3: ", misc_err_test_3))
print(paste("Train error for k=3: ", misc_err_train_3))
print(paste("Valid error for k=3: ", misc_err_valid_3))

```

## 5.
>Fit K-nearest neighbor classifiers to the training data for different values of K K = 1,2, â€¦ , 30, compute the error for the validation data as cross-entropy (when computing log of probabilities add a small constant within log, e.g. 1e-15, to avoid numerical problems) and plot the dependence of the validation error on the value of K K. 

Cross Entropy :
$$J(y,\hat{p}(y)) = -\sum_{i=1}^{n}\sum_{m=1}^{M}I(y_i=C_m)*log(\hat{p}(y_i=C_m))$$

```{r 1.5}
get_cross_entropy <- function(prob, true_vals){
  ce_sum <- 0
  for(i in 1:length(true_vals)){ #for each prediction
    truth <- true_vals[i] #Cm
    ce_sum <- ce_sum + log(prob[i,truth+1] + 1e-15)
  }
  return (-1*ce_sum)
}
k_values <- c()
x_ent_errs <- c()
for(i in 1:30){
  #Fitting the classifier
  cat(i, " ")
  #Valid:
  digits_kknn_ce <- kknn(formula = as.factor(V65) ~ . , train = digits_train, test = digits_valid, k = i, kernel = "rectangular")

  #Cross entropy Error:
  ce_err <- get_cross_entropy(digits_kknn_ce[["prob"]], digits_valid$V65)
  #Append to the vectors
  k_values <- append(k_values, i)
  x_ent_errs <- append(x_ent_errs, ce_err)
}
df_CE <- data.frame("k" = k_values, "CE_err" = x_ent_errs)
```

Plot:
```{r}
#Plotting misc_errs_test and misc_errs_train as a function of k
ggplot(df_CE, aes(x = k, y = CE_err)) + geom_line() + geom_point() + ggtitle("Cross Entropy error as a function of k") + xlab("k") + ylab("CE error")
```

>What is the optimal K K value here? Assuming that response has multinomial distribution, why might the cross-entropy be a more suitable choice of the error function than the misclassification error for this problem?

___

# Assignment 2
## 2.1
> Divide it into training and test data (60/40) and scale it appropriately. In the coming steps, assume that motor_UPDRS is normally distributed and is a function of the voice characteristics, and since the data are scaled, no intercept is needed in the modelling.

```{r 2.1}
# Read in data
df_park <- read.csv("parkinsons.csv")

# Split training/test and scale
set.seed(12345)
train_ind <- sample(1:nrow(df_park), floor(nrow(df_park) * 0.6))
df_train <- df_park[train_ind,]
df_test <- df_park[-train_ind,]

scaler <- preProcess(df_train)
df_train_scaled <- predict(scaler, df_train)
df_test_scaled <- predict(scaler, df_test)

```

## 2.2
> Compute a linear regression model from the training data, estimate training and test MSE and comment on which variables contribute significantly to the model.

```{r 2.2, echo=FALSE}
# Train model
mod <- lm(motor_UPDRS ~ . + 0 - subject. - age - sex - test_time - total_UPDRS, data=df_train_scaled) # https://stats.stackexchange.com/questions/143155/doing-multiple-regression-without-intercept-in-r-without-changing-data-dimensio

# Predict values
pred_train <- data.frame(pred=predict(mod, df_train_scaled), act=df_train_scaled$motor_UPDRS)
pred_test <- data.frame(pred=predict(mod, df_test_scaled), act=df_test_scaled$motor_UPDRS)

# Compute MSE
# https://www.statology.org/how-to-calculate-mse-in-r/
mse_train <- mean((pred_train$act - pred_train$pred)^2)
mse_test <- mean((pred_test$act - pred_test$pred)^2)

cat("MSE training data: ", mse_train, 
    "\nMSE test data: ", mse_test,
    "\n\n",
    sep="")
summary(mod)

```

The total_UPDRS variables has by far the biggest impact on the model. If this variable is included in the model, the MSE on both the training and test data are very good at about 0.09. The MSE on the test data is slightly higher than it is on the training data, which suggests the model was not overfit to the training data. However, when this variable is removed from the model, the MSE rises from 0.09 to about 0.8-0.9. There is no description of the variable included in the assignment, but it may be very strongly, if not fully correlated with motor_UPDRS, causing it to be the main predictor in the model. If this is the case, it is probably a bad idea to include this variable in the model, as it pretty much represents the same thing as the y variable. 

Either way, though, there are quite a few variables that are not significant. When including total_UPDRS in the model, Jitter.Abs., Jitter.DDP, Shimmer.APQ3, Shimmer.DDA, NHR, and HNR all have p-values above 0.7, and test_time, Jitter.PPQ5, Shimmer.dB., and DFA are also not significant. Not much pre-exploring of the data was done in the assignment before putting together the model, but it would be good to check if there is any multicollinearity happening within the Jitter and Shimmer categories. Similarly, NHR and HNR are similar measures and may be capturing the same information. 
When total_UPDRS is removed, Jiter.RAP, Jitter.DDP, and RPDE have very high p-values, and Jitter..., Jitter.PPQ5, Shimmer.dB., Shimmer.APQ3, and Shimmer.DDA are also not significant. While not being significant, Shimmer.APQ3 has the largest coefficient value, meaning it has the biggest impact on the prediction.

## 2.3a
> Loglikelihood function that for a given parameter vector ğœ½ and dispersion ğœ computes the log-likelihood function log ğ‘ƒ(ğ‘‡|ğœ½, ğœ) for the stated model and the training data.

```{r 2.3a}
loglikelihood <- function(data, formula, theta, sigma) {
  # Get variable names and y and x matrices
  y_var <- all.vars(formula)[1]
  x_var <- all.vars(formula)[2:length(all.vars(formula))]
  
  if ("." %in% x_var) {
    x_var_sub <- x_var[x_var != "."]
    x_var <- colnames(data)[colnames(data) != y_var]
    x_var <- x_var[!(x_var %in% x_var_sub)]
  }
  
  y <- data[, y_var]
  x <- data[, x_var]
  
  # Get n observations
  n <- nrow(data)
  
  # Compute log likelihood
  log_likelihood <- -n * log(sqrt(2 * pi * sigma^2)) - 1 / (2 * sigma^2) * sum((y - t(theta) * x)^2)
  
  return(log_likelihood)
}

```

## 2.3b
> Ridge function that for given vector ğœ½, scalar ğœ and scalar ğœ† uses function from 3a and adds up a Ridge penalty ğœ†â€–ğœ½â€–^2 to the minus log-likelihood.

```{r 2.3b}
ridge <- function(v_theta_sigma, data, formula, lambda) {
  theta <- v_theta_sigma[1:length(v_theta_sigma) - 1]
  sigma <- v_theta_sigma[length(v_theta_sigma)]
  
  penalty <- lambda * sum(theta^2) # https://stackoverflow.com/questions/10933945/how-to-calculate-the-euclidean-norm-of-a-vector-in-r
  min_loglikelihood <- -loglikelihood(data, formula, theta, sigma)
  penalized_min_loglikelihood <- min_loglikelihood + penalty
  
  return(penalized_min_loglikelihood)
}

```

## 2.3c
> RidgeOptfunction that depends on scalar ğœ†, uses function from 3b and function optim() with method=â€BFGSâ€ to find the optimal ğœ½ and ğœ for the given ğœ†.

```{r 2.3c}
ridge_opt <- function(data, formula, lambda) {
  # # https://stackoverflow.com/questions/24623488/how-do-i-use-a-function-with-parameters-in-optim-in-r
  # https://stackoverflow.com/questions/59517244/r-optim-can-i-pass-a-list-to-parameter-par
  y_var <- all.vars(formula)[1]
  x_var <- all.vars(formula)[2:length(all.vars(formula))]
  
  if ("." %in% x_var) {
    x_var_sub <- x_var[x_var != "."]
    x_var <- colnames(data)[colnames(data) != y_var]
    x_var <- x_var[!(x_var %in% x_var_sub)]
  }
  
  opt <- optim(c(rep(0, length(x_var)), 1), ridge, data=data, formula=formula, lambda=lambda, method="BFGS")
  opt_theta <- opt$par[1:length(opt$par) - 1]
  opt_sigma <- opt$par[length(opt$par)]
  
  return(list(theta=opt_theta, sigma=opt_sigma))
}

```

## 2.3d
> Df function that for a given scalar ğœ† computes the degrees of freedom of the Ridge model based on the training data.

```{r 2.3d}
df <- function(data, formula, lambda) {
  # Get x matrix
  y_var <- all.vars(formula)[1]
  x_var <- all.vars(formula)[2:length(all.vars(formula))]
  
  if ("." %in% x_var) {
    x_var_sub <- x_var[x_var != "."]
    x_var <- colnames(data)[colnames(data) != y_var]
    x_var <- x_var[!(x_var %in% x_var_sub)]
  }
  
  x <- as.matrix(data[, x_var])
  
  # Compute trace of hat matrix
  sum(diag((x %*% solve(t(x) %*% x + lambda * diag(ncol(x))) %*% t(x)))) # https://online.stat.psu.edu/stat508/lesson/5/5.1
}

```

## 2.4
> By using function RidgeOpt, compute optimal ğœ½ parameters for ğœ† = 1, ğœ† = 100 and ğœ† = 1000. Use the estimated parameters to predict the motor_UPDRS values for training and test data and report the training and test MSE values. Which penalty parameter is most appropriate among the selected ones? Compute and compare the degrees of freedom of these models and make appropriate conclusions.

```{r 2.4, echo=FALSE}
formula <- motor_UPDRS ~ . - subject. - age - sex - test_time - total_UPDRS

# Get opt theta and sigma for different lambdas
l_param1 <- ridge_opt(df_train_scaled, formula, lambda=1)
l_param100 <- ridge_opt(df_train_scaled, formula, lambda=100)
l_param1000 <- ridge_opt(df_train_scaled, formula, lambda=1000)

# Function for computing MSE
mse <- function(data, formula, theta) {
  # Get variable names and y and x matrices
  y_var <- all.vars(formula)[1]
  x_var <- all.vars(formula)[2:length(all.vars(formula))]
  
  if ("." %in% x_var) {
    x_var_sub <- x_var[x_var != "."]
    x_var <- colnames(data)[colnames(data) != y_var]
    x_var <- x_var[!(x_var %in% x_var_sub)]
  }
  
  y <- data[, y_var]
  x <- as.matrix(data[, x_var])
  
  y_hat <- x %*% theta
  
  # Compute MSE
  mean((y_hat - y)^2)
}

# Get y hat on training and test for different lambdas
mse_tr_1 <- mse(df_train_scaled, formula, l_param1$theta)
mse_te_1 <- mse(df_test_scaled, formula, l_param1$theta)

mse_tr_100 <- mse(df_train_scaled, formula, l_param100$theta)
mse_te_100 <- mse(df_test_scaled, formula, l_param100$theta)

mse_tr_1000 <- mse(df_train_scaled, formula, l_param1000$theta)
mse_te_1000 <- mse(df_test_scaled, formula, l_param1000$theta)

# Compute df for different models
df_tr_1 <- df(df_train_scaled, formula, 1)
df_te_1 <- df(df_test_scaled, formula, 1)

df_tr_100 <- df(df_train_scaled, formula, 100)
df_te_100 <- df(df_test_scaled, formula, 100)

df_tr_1000 <- df(df_train_scaled, formula, 1000)
df_te_1000 <- df(df_test_scaled, formula, 1000)

# Report output
cat("lambda = 1",
    "\nMSE train: ", mse_tr_1,
    "\nMSE test: ", mse_te_1,
    "\ndf train: ", df_tr_1,
    "\ndf test: ", df_te_1,
    "\n\n",
    "lambda = 100",
    "\nMSE train: ", mse_tr_100,
    "\nMSE test: ", mse_te_100,
    "\ndf train: ", df_tr_100,
    "\ndf test: ", df_te_100,
    "\n\n",
    "lambda = 1000",
    "\nMSE train: ", mse_tr_1000,
    "\nMSE test: ", mse_te_1000,
    "\ndf train: ", df_tr_1000,
    "\ndf test: ", df_te_1000,
    sep=""
)

```

Interestingly, the test MSE is a little bit better than the train MSE when lambda = 1. This could be due to a fortunate choice in train/test split, but it at least suggests this model is not overfitting to the data. However, this is not a reason to pick this model over the others, as both other models only show a small difference between the train and test MSE. This could be a signifier that the model could be more complex, but, again, it does show that neither model is overfitted to the data. The difference in MSE between the lambda = 100 and lambda = 1000 models is relatively small - only about 0.1 - compared to the difference between the lambda = 1 and lambda = 100 models - about 0.32. Between the lambda = 100 and lambda = 1000 models, the degrees of freedom go down a lot from around 9/10 to 5/5.5. These models perform almost equally in terms of MSE, but show a big difference in degrees of freedom. The most appropriate choice would probably be the model with lambda = 100, as its performance is good compared to the others, but the higher degrees of freedom give more statistical power. 

Reference: https://sites.utexas.edu/sos/degreesfreedom/

# Appendix
```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
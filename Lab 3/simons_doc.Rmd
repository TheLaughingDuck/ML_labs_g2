---
title: "simons_doc"
author: "Simon Jorstedt"
date: "`r Sys.Date()`"
output: pdf_document
---


# Assignment 3

```{r Setup, echo=F, message=F, warning=F}
# Setup

library(magrittr)
library(neuralnet)
library(dplyr)
```

## Assignment 3.1
In Figure 3.1 below, we see the result of a neural net applied to a set of $25$ datapoints uniformly spread over the interval $[0, 10]$, with the response being the sin function applied to the datapoints, with no added random error. These training points are colored dark green and made small. Red points are the pointwise predictions by the neural net. The blue curve represents the true sin curve.

```{r Train and visualise first NN, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.1

# Create sine data
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test

# Training a neural network
nn <- neuralnet(data = tr,
                formula = Sin ~ Var,
                hidden = c(10))

# Plot the training, test and predicted data
plot(tr, col="darkgreen",
     main = "Fig 3.1: Simple neural network fitting the sin function",
     xlab = "x",
     ylab = "sin(x)")
points(arrange(te, Var), type="l", col="blue")
points(tr$Var, nn$response, col="red", cex=2)

```

## Assignment 3.2
The neural net in Assignment 3.1 used a standard sigmoid activation function. Now we are going to use a linear activation function (or the identity) $h_1(x) = x$, the ReLu $h_2(x) = \max(0, x)$, and the softplus $h_3(x) = \ln(1+e^x)$ each in their own neural network. However, as we tried this, we ran into an issue with the ReLu. It does not appear to be preprogrammed in the `neuralnet` function, and neither does `neuralnet` seem to be able to handle the undifferentiable point in ReLu. Instead we will be using a function we call the "soft relu" $h_4(x) = \ln(1+e^x) \cdot \frac{e^x}{1+e^x}$, which is constructed by multiplying the softplus function with its derivative. That derivative happens also to be the sigmoid function. The result is a function that resembles the softplus, but that makes a less smooth turn around $x=0$, and thus approximates the ReLu better than the softplus.

The result is roughly the same as in Assignment 3.1, which was already optimal.

```{r nn training with three different activation functions, echo=F}
# Assignment 3.2

## NN with linear activation function
nn_linear <- neuralnet(data = tr,
                   formula = Sin ~ Var,
                   hidden = c(10),
                   act.fct = function(x){x})

## NN with soft relu activation function
soft_relu <- function(x){log(1+exp(x))*(exp(x))/(1+exp(x))}
nn_relu <- neuralnet(data = tr,
                     formula = Sin ~ Var,
                     hidden = c(10),
                     act.fct = soft_relu)

## NN with softplus activation function
nn_softplus <- neuralnet(data = tr,
                         formula = Sin ~ Var,
                         hidden = c(10),
                         act.fct = function(x){log(1+exp(x))})

```

```{r Figure 3.2 NN PLOTS, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.2

## Plot grid of neural network results
layout(matrix(c(1,2,3), nrow=1))

# Plot result of NN with linear activation
plot(tr, col="darkgreen",
     main = "Linear activation",
     xlab = "x",
     ylab = "sin(x)")
points(arrange(te, Var), type="l", col="blue")
points(tr$Var, nn_linear$response, col="red", cex=2)

# Plot result of NN with soft relu activation
plot(tr, col="darkgreen",
     main = "Soft relu activation",
     xlab = "x",
     ylab = "sin(x)")
points(arrange(te, Var), type="l", col="blue")
points(tr$Var, nn_relu$response, col="red", cex=2)

# Plot result of NN with softplus activation
plot(tr, col="darkgreen",
     main = "Softplus activation",
     xlab = "x",
     ylab = "sin(x)")
points(arrange(te, Var), type="l", col="blue")
points(tr$Var, nn_softplus$response, col="red", cex=2)

# Set figure title
mtext("Figure 3.2: Three activation function", side=3, outer=TRUE, line=-1)
```

## Assignment 3.3
Now we generate data similarly to before in the range $[0,50]$ and attempt to predict the value of the $\sin$ function applied to these values using the Neural Network that was trained in Assignment 3.1. Essentially we will use the Neural Network to predict the response of data from far outside the range of the training data. The result of this is plotted in Figure 3.3. Again the training data of the model is represented by green circles, and the predictions are represented by the red curve. The blue curve represents the entire new dataset (essentially the sin function graph). Clearly, the Neural Network has a fatal flaw: It breaks away, producing terrible predictions immediately outside the range of the training data.

```{r Figure 3.3, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.3

set.seed(9378364)
Var <- runif(500, 0, 50)
mydata_0_to_50 <- data.frame(Var, Sin=sin(Var))

# Plot NN fitted values to the new data
mydata_0_to_50 <- mydata_0_to_50 %>%
  mutate(nn_prediction = predict(nn, data.frame(mydata_0_to_50$Var))) %>%
  arrange(Var)

plot(x = mydata_0_to_50$Var,
     y = mydata_0_to_50$nn_prediction,
     col="red", type="l", cex=2,
     main = "Fig 3.3: Neural network predictions on new data from outside the\nrange of training data",
     xlab = "x",
     ylab = "sin(x)")
points(tr %>% arrange(Var), col="darkgreen", type = "p", lwd=1.5)
points(arrange(mydata_0_to_50, Var), type="l", col="blue", lwd=2)


# Find the limit of the NN as x increases
# This had to be hardcoded because the neuralnet function has its parameters
# structured in a quite unintuitive way.
sign_of_W1 <- nn$result[c(5,7,9,11,13,15,17,19,21,23)] > 0
limit <- sum(sign_of_W1 * nn$result[25:34]) + nn$result[24]
abline(b=0, a=limit, col="gold")
```

## Assignment 3.4
In Assignment 3.3 we saw that the prediction curve of the Neural Network appeared to converge to some value. To find this limit, we formulate the function $f(x)$ of the Neural Network, first in compact notation, and then as a linear combination.

$$f(x) = \boldsymbol{W}^{(2)} \sigma \Big( \boldsymbol{W}^{(1)}x + \boldsymbol{b}^{(1)} \Big) + b^{(2)} = \boldsymbol{W}^{(2)}_1 \sigma \Big( \boldsymbol{W}^{(1)}_1 x + \boldsymbol{b}^{(1)}_1 \Big) + \cdots + \boldsymbol{W}^{(2)}_{10} \sigma \Big( \boldsymbol{W}^{(1)}_{10} x + \boldsymbol{b}^{(1)}_{10} \Big) + b^{(2)}$$

When studying this function, we find that if $\boldsymbol{W}^{(2)}_i$ is non-negative, then $\sigma \Big( \boldsymbol{W}^{(1)}_i x + \boldsymbol{b}^{(1)}_i \Big)$ will approach $1$ as $x$ becomes larger. If $\boldsymbol{W}^{(2)}_i$ is negative, then $\sigma \Big( \boldsymbol{W}^{(1)}_i x + \boldsymbol{b}^{(1)}_i \Big)$ will approach $0$ as $x$ becomes larger. This has the effect that as $x \to \infty$, $f(x)$ reduces to the following, where $\delta_i$ is $0$ if $\boldsymbol{W}^{(2)}_i < 0$ and $1$ otherwise.

$$f(x) = \boldsymbol{W}^{(2)}_1 \cdot \delta_1 + \cdots + \boldsymbol{W}^{(2)}_{10} \cdot \delta_{10} + b^{(2)}$$

We calculate this limit for the trained Neural Network from Assignment 3.1, and include the limit as a gold line in Figure 3.3.


```{r, eval=F}
# Random initialization of the weights in the interval [-1, 1]
winit <- # Your code here
nn <- neuralnet(# Your code here)

# Plot of the training data (black), test data (blue), and predictions (red)

plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1
```


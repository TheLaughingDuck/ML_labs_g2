---
title: "lab2"
author: "Hugo Morvan & Marijn Jaarsma & Simon Jorstedt "
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(glmnet)
library(ggplot2)
```
## Statement of Contribution:

* Assignment 1 was coded and analysed by Hugo
* Assignment 2 was coded and analysed by 
* Assignment 3 was coded and analysed by
All code and questions were analysed and discussed together.

## Assignment 1. Explicit regularization

The tecator.csv contains the results of study aimed to investigate whether a near infrared absorbance spectrum can be used to predict the fat content of samples of meat. For each meat sample the data consists of a 100 channel spectrum of absorbance records and the levels of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The moisture, fat and protein are determined by analytic chemistry.

Divide data randomly into train and test (50/50) by using the codes from the lectures.

```{r data1}
# 1.0
data=read.csv("tecator.csv")

n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]

```

### 1.1
1. Assume that Fat can be modeled as a linear regression in which absorbance characteristics (Channels) are used as features. Report the underlying probabilistic model, fit the linear regression to the training data and estimate the training and test errors. Comment on the quality of fit and prediction and therefore on the quality of model.

```{r 1.1, warning=FALSE}
# 1.1
lin_mod <- lm(Fat ~ . -Moisture -Protein, data = train)
#summary(lin_mod)

predic_train <- predict.lm(lin_mod, train)
train_error <- mean((train$Fat - predic_train)^2)
cat("train error (using predict()):", train_error, "\n")
fitted_train <- fitted(lin_mod, train)
train_error2 <- mean((train$Fat - fitted_train)^2)
cat("train error (using fitted()):", train_error2, "\n")

predic_test <- predict.lm(lin_mod, test)
test_error <- mean((test$Fat - predic_test)^2)
cat("test error (using predic()):", test_error, "\n")
fitted_test <- fitted(lin_mod, test)
test_error2 <- mean((test$Fat - fitted_test)^2)
cat("test error (using fitted()):", test_error2, "\n")

```
Test results are awful, is is probably overfitting. Also the dataset size is very small (215 observations) therefore test and train are both ~100 observations, which is barely more than the number of features (100).

### 1.2
2. Assume now that Fat can be modeled as a LASSO regression in which all Channels are used as features. Report the cost function that should be optimized in this scenario.

The cost function that should be optimized is the following:
$$\hat{\theta}^{lasso} = argmin\{\frac{1}n \sum_{i=1}^{n}(y_i-\theta_0-\theta_{1}x_{1i}-...-\theta_px_{pi})^2+\lambda\sum_{j=1}^{p}|\theta_j|\}$$

### 1.3
3. Fit the LASSO regression model to the training data. Present a plot illustrating how the regression coefficients depend on the log of penalty factor (log λ) and interpret this plot. 

```{r 1.3}
# 1.3
lasso <- glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 1)
plot(lasso, xvar = "lambda", label = TRUE)
#add an x=0.4 line
abline(v = -0.3, col = "red")

```
Interpret this plot: the higher the penalty factor, the more coefficients are set to zero.
What value of the penalty factor can be chosen if we want to select a model with only three features?
At log(lambda) = -0.3, there are only 3 non-zero features left, Hence we can choose $\lambda = e^{-0.3} = 0.74$.

### 1.4
4. Repeat step 3 but fit Ridge instead of the LASSO regression and compare the plots from steps 3 and 4. Conclusions?

```{r 1.4}
# 1.4
ridge <- glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 0)
plot(ridge, xvar = "lambda", label = TRUE)

```
The higher the penalty factor, the smaller the coefficients. The coefficients are never set to zero, unlike in the LASSO regression.
### 1.5

 5. Use cross-validation with default number of folds to compute the optimal LASSO model. Present a plot showing the dependence of the CV score on $\log\lambda$ and comment how the CV score changes with $\log\lambda$. 

```{r 1.5.1}
# 1.5
cv_lasso <- cv.glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 1)
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min
cat("best lambda:", best_lambda, "\n")
```
Report the optimal $\lambda$ and how many variables were chosen in this model. 
The optimal $\lambda$ is 0.05744535 or log(lambda) = -2.86. 
```{r}
coef(cv_lasso, s = "lambda.min")

```
9 variables were chosen in this model.


Does the information displayed in the plot suggests that the optimal $\lambda$ value results in a statistically significantly better prediction than $\log\lambda = −4$ ? 

*** check this ***
The information displayed in the plot suggests that the optimal lambda has same prediction MSE as log(lambda) = -4.
*** check this ***

Finally, create a scatter plot of the original test versus predicted test values for the model corresponding to optimal lambda and comment whether the model predictions are good.

```{r 1.5.2, echo=FALSE, message=FALSE, warning=FALSE}
predic_test <- predict(cv_lasso, newx = as.matrix(test[,1:100]), s = "lambda.min")
plot(test$Fat, predic_test) + abline(1, 1)

```
The model predictions seems to be good as the points are close to the line y=x. 

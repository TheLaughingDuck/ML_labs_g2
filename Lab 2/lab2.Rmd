---
title: "lab2"
author: "Hugo Morvan & Marijn Jaarsma & Simon Jorstedt "
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(glmnet)
library(ggplot2)
library(tree)
library(rpart)

```

## Statement of Contribution:

* Assignment 1 was coded and analysed by Hugo
* Assignment 2 was coded and analysed by Marijn
* Assignment 3 was coded and analysed by Simon
All code and questions were analysed and discussed together.

## Assignment 1. Explicit regularization

The tecator.csv contains the results of study aimed to investigate whether a near infrared absorbance spectrum can be used to predict the fat content of samples of meat. For each meat sample the data consists of a 100 channel spectrum of absorbance records and the levels of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The moisture, fat and protein are determined by analytic chemistry.

Divide data randomly into train and test (50/50) by using the codes from the lectures.

```{r data1}
# 1.0
data=read.csv("tecator.csv")

n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]

```

### 1.1
1. Assume that Fat can be modeled as a linear regression in which absorbance characteristics (Channels) are used as features. Report the underlying probabilistic model, fit the linear regression to the training data and estimate the training and test errors. Comment on the quality of fit and prediction and therefore on the quality of model.

```{r 1.1, warning=FALSE}
# 1.1
lin_mod <- lm(Fat ~ . -Moisture -Protein, data = train)
#summary(lin_mod)

predic_train <- predict.lm(lin_mod, train)
train_error <- mean((train$Fat - predic_train)^2)
cat("train error (using predict()):", train_error, "\n")
fitted_train <- fitted(lin_mod, train)
train_error2 <- mean((train$Fat - fitted_train)^2)
cat("train error (using fitted()):", train_error2, "\n")

predic_test <- predict.lm(lin_mod, test)
test_error <- mean((test$Fat - predic_test)^2)
cat("test error (using predic()):", test_error, "\n")
fitted_test <- fitted(lin_mod, test)
test_error2 <- mean((test$Fat - fitted_test)^2)
cat("test error (using fitted()):", test_error2, "\n")

```
Test results are awful, is is probably overfitting. Also the dataset size is very small (215 observations) therefore test and train are both ~100 observations, which is barely more than the number of features (100).

### 1.2
2. Assume now that Fat can be modeled as a LASSO regression in which all Channels are used as features. Report the cost function that should be optimized in this scenario.

The cost function that should be optimized is the following:
$$\hat{\theta}^{lasso} = argmin\{\frac{1}n \sum_{i=1}^{n}(y_i-\theta_0-\theta_{1}x_{1i}-...-\theta_px_{pi})^2+\lambda\sum_{j=1}^{p}|\theta_j|\}$$

### 1.3
3. Fit the LASSO regression model to the training data. Present a plot illustrating how the regression coefficients depend on the log of penalty factor (log λ) and interpret this plot. 

```{r 1.3}
# 1.3
lasso <- glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 1)
plot(lasso, xvar = "lambda", label = TRUE)
#add an x=0.4 line
abline(v = -0.3, col = "red")

```
Interpret this plot: the higher the penalty factor, the more coefficients are set to zero.
What value of the penalty factor can be chosen if we want to select a model with only three features?
At log(lambda) = -0.3, there are only 3 non-zero features left, Hence we can choose $\lambda = e^{-0.3} = 0.74$.

### 1.4
4. Repeat step 3 but fit Ridge instead of the LASSO regression and compare the plots from steps 3 and 4. Conclusions?

```{r 1.4}
# 1.4
ridge <- glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 0)
plot(ridge, xvar = "lambda", label = TRUE)

```
The higher the penalty factor, the smaller the coefficients. The coefficients are never set to zero, unlike in the LASSO regression.
### 1.5

 5. Use cross-validation with default number of folds to compute the optimal LASSO model. Present a plot showing the dependence of the CV score on $\log\lambda$ and comment how the CV score changes with $\log\lambda$. 

```{r 1.5.1}
# 1.5
cv_lasso <- cv.glmnet(as.matrix(train[,1:100]), train$Fat, alpha = 1)
plot(cv_lasso)
best_lambda <- cv_lasso$lambda.min
cat("best lambda:", best_lambda, "\n")
```
Report the optimal $\lambda$ and how many variables were chosen in this model. 
The optimal $\lambda$ is 0.05744535 or log(lambda) = -2.86. 
```{r}
coef(cv_lasso, s = "lambda.min")

```
9 variables were chosen in this model.


Does the information displayed in the plot suggests that the optimal $\lambda$ value results in a statistically significantly better prediction than $\log\lambda = −4$ ? 

*** check this ***
The information displayed in the plot suggests that the optimal lambda has same prediction MSE as log(lambda) = -4.
*** check this ***

Finally, create a scatter plot of the original test versus predicted test values for the model corresponding to optimal lambda and comment whether the model predictions are good.

```{r 1.5.2, echo=FALSE, message=FALSE, warning=FALSE}
predic_test <- predict(cv_lasso, newx = as.matrix(test[,1:100]), s = "lambda.min")
plot(test$Fat, predic_test) + abline(1, 1)

```
The model predictions seems to be good as the points are close to the line y=x. 

# Assignment 2: Decision trees and logistic regression for bank marketing

## 2.1

Import the data to R, remove variable "duration" and divide into
training/validation/test as 40/30/30: use data partitioning code specified in
Lecture 2a.

```{r 2.1}
# 2.1
# Read in data, remove 'duration' column, and add 1/0 bool column for y
df_bank <- read.csv("bank-full.csv", sep=";")
df_bank <- df_bank[, names(df_bank) != "duration"]

# Set categorical column as factor for modeling
# https://stackoverflow.com/questions/54768433/nas-introduced-by-coercion-error-when-using-tree-function
# https://www.listendata.com/2015/05/converting-multiple-numeric-variables.html
df_bank[sapply(df_bank, is.character)] <- lapply(df_bank[sapply(df_bank, is.character)], as.factor)

# Train/validation/test split
n <- nrow(df_bank)
set.seed(12345)
id_train <- sample(1:n, floor(n * 0.4))
df_train <- df_bank[id_train,]

id_rest <- setdiff(1:n, id_train)
set.seed(12345)
id_valid <- sample(id_rest, floor(n * 0.3))
df_valid <- df_bank[id_valid,]

id_test <- setdiff(id_rest, id_valid)
df_test <- df_bank[id_test,]

```

## 2.2

Fit decision trees to the training data so that you change the default settings
one by one (i.e. not simultaneously): a. Decision Tree with default settings. b.
Decision Tree with smallest allowed node size equal to 7000. c. Decision trees
minimum deviance to 0.0005. and report the misclassification rates for the
training and validation data. Which model is the best one among these three?
Report how changing the deviance and node size affected the size of the trees
and explain why.

```{r 2.2}
# 2.2a
n <- nrow(df_train)
mod1 <- tree(y ~ ., data=df_train)
pred_train1 <- predict(mod1, newdata=df_train, type="class")
pred_valid1 <- predict(mod1, newdata=df_valid, type="class")
misclass_train1 <- (table(df_train$y, pred_train1)["no", "yes"] + table(df_train$y, pred_train1)["yes", "no"]) / nrow(df_train)
misclass_valid1 <- (table(df_valid$y, pred_valid1)["no", "yes"] + table(df_valid$y, pred_valid1)["yes", "no"]) / nrow(df_valid)

# plot(mod1)
# text(mod1, pretty=0)

# 2.2b
# https://search.r-project.org/CRAN/refmans/tree/html/tree.control.html
mod2 <- tree(y ~ ., data=df_train, control=tree.control(nobs=n, minsize=7000))
pred_train2 <- predict(mod2, newdata=df_train, type="class")
pred_valid2 <- predict(mod2, newdata=df_valid, type="class")
misclass_train2 <- (table(df_train$y, pred_train2)["no", "yes"] + table(df_train$y, pred_train2)["yes", "no"]) / nrow(df_train)
misclass_valid2 <- (table(df_valid$y, pred_valid2)["no", "yes"] + table(df_valid$y, pred_valid2)["yes", "no"]) / nrow(df_valid)

# plot(mod2)
# text(mod2, pretty=0)

# 2.2c
# https://search.r-project.org/CRAN/refmans/tree/html/tree.control.html
mod3 <- tree(y ~ ., data=df_train, control=tree.control(nobs=n, mindev=0.0005))
pred_train3 <- predict(mod3, newdata=df_train, type="class")
pred_valid3 <- predict(mod3, newdata=df_valid, type="class")
misclass_train3 <- (table(df_train$y, pred_train3)["no", "yes"] + table(df_train$y, pred_train3)["yes", "no"]) / nrow(df_train)
misclass_valid3 <- (table(df_valid$y, pred_valid3)["no", "yes"] + table(df_valid$y, pred_valid3)["yes", "no"]) / nrow(df_valid)

# plot(mod3)
# text(mod3, pretty=0)

# Report values
cat("Misclassification rate model a:\n", 
    "\tTrain: ", misclass_train1, "\n",
    "\tValidation: ", misclass_valid1, "\n\n",
    "Misclassification rate model b:\n",
    "\tTrain: ", misclass_train2, "\n",
    "\tValidation: ", misclass_valid2, "\n\n",
    "Misclassification rate model c:\n",
    "\tTrain: ", misclass_train3, "\n",
    "\tValidation: ", misclass_valid3,
    sep="")

```

Model a and b are incredibly close to each other in performance. Both of their
validation errors are better than model c, but since model b has one set of
terminal nodes less than model a this model is less complex and thus the better
option between the two.

Setting the minimal node size to 7000 in model b means resulted in the pruning
of one branch. The split that occurred here would have resulted in a node with
less than 7000 observations, which was not allowed with this option.

The default minimum deviance is 0.01
(https://cran.r-project.org/web/packages/tree/tree.pdf). Setting the
minimum deviance at a much smaller number (0.0005) means that more branches are
allowed and results in a much bigger tree.

## 2.3

Use training and validation sets to choose the optimal tree depth in the model
2c: study the trees up to 50 leaves. Present a graph of the dependence of
deviances for the training and the validation data on the number of leaves and
interpret this graph in terms of bias-variance tradeoff. Report the optimal
amount of leaves and which variables seem to be most important for decision
making in this tree. Interpret the information provided by the tree structure
(not everything but most important findings).

```{r 2.3}
# 2.3
train_score <- rep(0, 50)
valid_score <- rep(0, 50)

for (i in 2:50) {
  pruned_tree <- prune.tree(mod3, best=i)
  pred <- predict(pruned_tree, newdata=df_valid, type="tree")
  train_score[i] <- deviance(pruned_tree)
  valid_score[i] <- deviance(pred)

  # plot(pruned_tree)
  # text(pruned_tree, pretty=0)
  # title(paste0("k=", i))
}

# https://r-coder.com/add-legend-r/
plot(2:50, train_score[2:50], type="b", col="red", ylim=c(8000, 12000), xlab="# leaves", ylab="Deviance")
points(2:50, valid_score[2:50], type="b", col="blue")
legend(x="topright", legend=c("Train", "Validation"), lty=c(1, 1), col=c("red", "blue"))
title(paste0("Opt. # leaves: ", match(min(valid_score[2:50]), valid_score)))

opt_pruned_tree <- prune.tree(mod3, best=22)
plot(opt_pruned_tree)
text(opt_pruned_tree, pretty=0)

```

The number of leaves that produces the lowest deviance on the validation set is 22. Interestingly, the deviance on the training data is higher than that on the validation data for all values of k. If k goes beyond 50, they get closer to each other, but validation will always remain lower. Though it should also be tested on the training data, this at least means the model is not overfitting. The poutcome variable is the first split, after which pdays is used for a split followed by entrepeneur. On the left side of the tree, where most decisions lead to "no", poutcome is followed by month, contact, month again, and pdays to lead to a yes decision. As the tree gets bigger it becomes difficult to read the overlapping text, but month seems to be an important variable that comes back often, and pdays a variable that leads to a split between "yes" and "no". 

As the number of leaves on the tree increases, the bias of the model gets reduced. However, the variance also goes up as the deviance of the validation set increases with a more complex model, signaling overfitting. Purely aiming to improve the bias of the model, then, is not necessarily a good thing if this means the variance is not staying constant. Other methods, such as bagging or random forests, may be applied to improve one of these while keeping the other constant (or at least not increasing the other enough for an undesirable effect), but in the case of a simple decision tree such as we have here, the sweet spot is somewhere in the middle.

## 2.4

Estimate the confusion matrix, accuracy and F1 score for the test data by using
the optimal model from step 3. Comment whether the model has a good predictive
power and which of the measures (accuracy or F1-score) should be preferred here.

```{r 2.4}
# 2.4
# Find number of leaves with minimum validation score
best_k <- match(min(valid_score[2:length(valid_score)]), valid_score)
pruned_mod <- prune.tree(mod3, best=best_k)
pred_test <- predict(pruned_mod, newdata=df_test, type="class")

# Confusion matrix
# https://datascience.stackexchange.com/questions/27132/decision-tree-used-for-calculating-precision-accuracy-and-recall-class-breakd
conf_mat <- table(df_test$y, pred_test, dnn=c("True", "Pred"))

fun_scoring <- function(conf_mat) {
  acc <- (conf_mat["yes", "yes"] + conf_mat["no", "no"]) / sum(conf_mat)
  prec <- conf_mat["yes", "yes"] / (conf_mat["yes", "yes"] + conf_mat["no", "yes"])
  rec <- conf_mat["yes", "yes"] / (conf_mat["yes", "yes"] + conf_mat["yes", "no"])
  F1 <- (2 * prec * rec) / (prec + rec)
  
  cat("Number of leaves: ", best_k, "\n",
      "Accuracy: ", round(acc, 3), "\n",
      "F1: ", round(F1, 3), "\n\n",
      "Confusion matrix:\n",
      sep="")
  conf_mat
}

fun_scoring(conf_mat)

```

The model is quite good at prediction "no", but bad at prediction "yes". There
are much fewer "yes" values in the data (1585 versus 11979 "no"), but the model
predicts many false negatives (1371 out of 1585). The accuracy measure would
suggest this model performs very well, but if we really care about correctly
predicting "yes" values the F1 measure would be a better measure to consider.
This takes into account precision and recall, so it works better on an
imbalanced dataset. The F1 score for this model is quite low (0.225), suggesting
that it is bad at accurately predicting positive values. This can also be seen
from the confusion matrix. If we do not care much about accurately predicting
"yes" values, we can accept the accuracy rate as a good measure, but considering
the imbalanced nature of the data, F1 would be preferred.

## 2.5

Perform a decision tree classification of the test data with the following loss
matrix, $$L=\begin{pmatrix}
0 & 5\\
1 & 0
\end{pmatrix}$$ and report the confusion matrix for the test data. Compare the
results with the results from step 4 and discuss how the rates has changed and
why.

```{r 2.5}
# 2.5
# Ordered differently according to the documentation
# https://stackoverflow.com/questions/49646377/loss-matrix-in-rs-package-rpart
loss_matrix <- matrix(c(0, 5, 1, 0), nrow=2, ncol=2)
mod4 <- rpart(y ~ ., data=df_train, method="class", parms=list(loss=loss_matrix))
pred_test4 <- predict(mod4, newdata=df_test, type="class")
conf_mat <- table(df_test$y, pred_test4, dnn=c("True", "Pred"))
fun_scoring(conf_mat)

```

Using the loss matrix, the true positive rate has gone up, along with the false positive rate. The false negative rate has gone down slightly, but is still not very good compared to the true positive rate. As we are penalizing false negative rate by five times what we are penalizing false positives with, the model is now much more likely to predict "yes". In combination with the unbalanced nature of the data, this has led to a highly increase false positive rate, rather than a better true positive rate.

## 2.6

Use the optimal tree and a logistic regression model to classify the test data
by using the following principle:
$$\hat{Y}=yes\ if\ p(Y='yes'|X) > \pi,\ otherwise\ \hat{Y}=no$$ where
$pi=0.05, 0.1, 0.15, ..., 0.9, 0.95$. Compute the TPR and FPR values for the two
models and plot the corresponding ROC curves. Conclusion? Why precision-recall
curve could be a better option here?

```{r 2.6}
# 2.6
v_pi <- seq(from=0.05, to=0.95, by=0.05)
log_mod <- glm(y ~ ., family=binomial(link="logit"), data=df_train)
pred_log_prob <- predict(log_mod, newdata=df_train, type="response")
pred_tree_prob <- predict(mod4, newdata=df_train, type="prob")

v_fpr_log <- vector()
v_tpr_log <- vector()
v_fpr_tree <- vector()
v_tpr_tree <- vector()
for (pi in v_pi) {
  pred_log <- ifelse(pred_log_prob > pi, "yes", "no")
  pred_tree <- ifelse(pred_tree_prob[, "yes"] > pi, "yes", "no")
  
  fp_log <- length(pred_log[(pred_log == "yes") & (df_train$y == "no")])
  fn_log <- length(pred_log[(pred_log == "no") & (df_train$y == "yes")])
  tp_log <- length(pred_log[(pred_log == "yes") & (df_train$y == "yes")])
  tn_log <- length(pred_log[(pred_log == "no") & (df_train$y == "no")])
  v_fpr_log <- append(v_fpr_log, fp_log / (fp_log + tn_log))
  v_tpr_log <- append(v_tpr_log, tp_log / (tp_log + fn_log))
  
  fp_tree <- length(pred_tree[(pred_tree == "yes") & (df_train$y == "no")])
  fn_tree <- length(pred_tree[(pred_tree == "no") & (df_train$y == "yes")])
  tp_tree <- length(pred_tree[(pred_tree == "yes") & (df_train$y == "yes")])
  tn_tree <- length(pred_tree[(pred_tree == "no") & (df_train$y == "no")])
  v_fpr_tree <- append(v_fpr_tree, fp_tree / (fp_tree + tn_tree))
  v_tpr_tree <- append(v_tpr_tree, tp_tree / (tp_tree + fn_tree))
}

# Plot ROC logistic regression
plot(v_fpr_log, v_tpr_log, type="l", xlim=c(0, 1), ylim=c(0, 1))
lines(c(0, 1), c(0, 1))
title("ROC Logistic Regression")

# Plot ROC decision tree
plot(v_fpr_tree, v_tpr_tree, type="l", xlim=c(0, 1), ylim=c(0, 1))
lines(c(0, 1), c(0, 1))
title("ROC Decision Tree")

```

Looking purely at the ROC curves, it seems that the logistic regression performs slightly better than the decision tree. However, considering the unbalanced nature of the data, it might be better to consider a precision-recall curve. 



---
title: "Simons_document"
author: "Simon Jorstedt"
date: "`r Sys.Date()`"
output: pdf_document
---


# Assignment 3

## Setup

```{r}
# Setup

library(magrittr)
library(dplyr)
library(caret)
library(ggplot2)

crime_df <- read.csv("communities.csv")

```

## Assignment 3.1
We are given some crime data. After excluding the response variable ViolentCrimesPerPop we analyse it using PCA, and find that the first component (PC1) explains about $25.017$ % of the variance, and the second component (PC2) explains about $16.936$ % of the variance in the data. To achieve $95$ % variance explained, the first 35 principal components must be used.

```{r, echo=F}
# Assignment 3.1

# Establish a scaler for the entire dataset
scaler_crime1 <- crime_df %>%
  select(-"ViolentCrimesPerPop") %>%
  preProcess()

# Rescale the entire dataset and create X_crime
X_crime <- crime_df %>%
  select(-"ViolentCrimesPerPop") %>%
  predict(scaler_crime1, .) %>%
  as.matrix()

# Compute sample covariance matrix for X_crime
S_crime <- (1/nrow(X_crime)) * t(X_crime) %*% X_crime

# Calculate eigenvalues for the sample covariance matrix
U_crime <- eigen(S_crime)

# Calculate the percentage of variance explained by each PC
var_explained <- (100*U_crime$values / sum(U_crime$values)) %>%
  round(digits = 3)

cat("Variance explained by PC1:", var_explained[1] %>% sum(), "%\n")

cat("Variance explained by PC2:", var_explained[2] %>% sum(), "%\n")

# 35 PC's needed to achieve at least 95% variance explained
cat("Variance explained by first 35 PC's:",
    var_explained[1:35] %>% sum(), "%\n")

```

## Assignment 3.2
Now we repeat the PCA performed in Assignment 3.1 using prebuilt R functionality. We find that the contributions to PC1 are fairly well spread across the variables, and the five with largest (absolute) contribution are very close in their contribution. There is clearly underlying dependencies among these variables. Median family income and median income are of course highly correlated. The other variables are also likely highly explanatory for income and wealth in general.

```{r, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.2

PCA_2 <- princomp(crime_df %>% select(-"ViolentCrimesPerPop") %>% scale())

# U contains the PC loadings
# The eigenvectors basically (columnwise)
U <- PCA_2$loadings

# Plot a trace plot for
plot(U[,1], main="Fig 3.2. Traceplot, PC1")

largest_contributors <- U[,1] %>% abs() %>% sort(decreasing = T) %>% names()
cat("Five largest contributors to PC1 are:\n")
cat(largest_contributors[1:5], sep = "\n")

```

Next, in Figure 3.3 we plot the PC scores of PC1 and PC2, and map the color of points to the Violent Crimes per Population feature.

```{r, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.2

Z_crime_2 <- X_crime %*% U_crime$vectors[,c(1,2)] %>%
  as.data.frame() %>%
  cbind(ViolentCrimesPerPop = crime_df$ViolentCrimesPerPop) %>%
  cbind(testcolor = c(1:1994))

ggplot(data = Z_crime_2,
       aes(x = Z_crime_2[,1],
           y = Z_crime_2[,2],
           col = ViolentCrimesPerPop))
  geom_point() +
  ggtitle("Fig 3.3. Loadings")

```

## Assignment 3.3

```{r, echo=F}
# Assignment 3.3

## Split data into train and test
n = nrow(crime_df)
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=crime_df[id,]
test=crime_df[-id,]

```

```{r}
# Assignment 3.3

# Establish Scaling
scaling <- train %>% preProcess()

# Scale train and test data
train <- predict(scaling, train)
test <- predict(scaling, test)

# Train lin reg model
model1 <- lm(data = train,
             formula = ViolentCrimesPerPop ~ .)

# Compute train error (MSE)
train_mse <- mean((train$ViolentCrimesPerPop - model1$fitted.values)^2)

# Predict new responses for the test data
test_pred <- predict(model1,
                     test[,-which(names(test) == "ViolentCrimesPerPop")])
test_mse <- mean((test$ViolentCrimesPerPop - test_pred)^2)

cat("Train mse:", train_mse, "\n")
cat("Test mse:", test_mse, "\n")

```

## Assignment 3.4

```{r}
## Assignment 3.4

train_noresponse <- train[,-which(names(train) == "ViolentCrimesPerPop")]
y_true_train <- train[,which(names(train) == "ViolentCrimesPerPop")]

test_noresponse <- test[,-which(names(test) == "ViolentCrimesPerPop")]
y_true_test <- test[,which(names(test) == "ViolentCrimesPerPop")]

train_errors <- c()
test_errors <- c()
test_prints <- c(3)

# cost function
# data should NOT contain the response variable
# No intercept is included in the underlying model
cost_linreg <- function(theta){
  
  # Calculate train error
  y_pred_train <- as.matrix(train_noresponse) %*% theta
  train_cost <- mean((y_pred_train - y_true_train)^2)
  train_errors <<- c(train_errors, train_cost)
  
  # Calculate test error
  y_pred_test <- as.matrix(test_noresponse) %*% theta
  test_cost <- mean((y_pred_test - y_true_test)^2)
  test_errors <<- c(test_errors, test_cost)
  
  return(train_cost)
}

optim_object <- optim(rep(0, 100), method="BFGS", fn = cost_linreg, control = list(trace=T))

cat(test_prints, "\n")
```


```{r}
# Iterate and stuff
# theta <- rep(0, 100)
# train_errors <- c()
# test_errors <- c()
# for(i in 1:1000){
#   cat(i, " ")
#   optim_object <- optim(theta, method="BFGS", fn = cost_linreg)
#   
#   # Compute and store train error
#   train_prediction <- as.matrix(train_noresponse) %*% optim_object$par
#   train_errors <- c(train_errors, mean((train_prediction - y_true)^2))
#   
#   # Compute and store test error
#   test_noresponse <- test[,-which(names(test) == "ViolentCrimesPerPop")]
#   test_prediction <- as.matrix(test_noresponse) %*% optim_object$par
#   test_errors <- c(test_errors, mean((test_prediction - y_true)^2))
#   
#   # Update theta
#   theta <- optim_object$par
# }

ylim <- c(min(train_errors[-c(1:500)], test_errors[-c(1:500)]), max(train_errors[-c(1:500)], test_errors[-c(1:500)]))
plot(test_errors[-c(1:500)], type="l", col="red", ylim = ylim)
points(train_errors[-c(1:500)], type="l", col="blue")
```



# Questions
What is the difference between `prcomp` and `princomp`? Why do we redo the PCA after we have already done it in 3.1?

Why are all the diagonal elements equal in the sample covariance matrix? Because we first rescale the data before computing the sample covariance matrix. When we rescale the data, it is done such that every column (variable) gets mean 0, and variance 1.



# PCA MATH TESTING

```{r}
X_iris <- iris[,-5] %>% scale() # Drop Species col

# Calculate sample covariance matrix
S_iris <- 1/nrow(X_iris) * t(X_iris) %*% X_iris # Possibly divide by nrow(X_iris)-1

U_iris <- eigen(S_iris)

# Control calculation for the Eigenvalues (It worked!)
#S_iris %*% U_iris$vectors[,1]
#U_iris$values[1] * U_iris$vectors[,1]

# Calculate the new coordinates of the observations when using PC1 and PC2
# Also called scores
Z_iris_2 <- X_iris %*% U_iris$vectors[,c(1,2)]

# Plot new coordinates
plot(Z_iris_2)
```

# Old code

```{r}
# -V- NOT REQUIRED -V-

# Calculate loadings, or scores, for PC1 and PC2
#Z_crime_2 <- X_crime %*% U_crime$vectors[,c(1,2)]

#plot(Z_crime_2)

# Calculate approximate original data
#X_crime_approx <- Z_crime_2 %*% t(U_crime$vectors[,c(1,2)]) + scaler$mean

# -^- NOT REQUIRED -^-
```



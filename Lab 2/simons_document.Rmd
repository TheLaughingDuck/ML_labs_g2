---
title: "Simons_document"
author: "Simon Jorstedt"
date: "`r Sys.Date()`"
output: pdf_document
---


# Assignment 3

## Setup

```{r, message=F, warning=F}
# Setup

library(magrittr)
library(dplyr)
library(caret)
library(ggplot2)

crime_df <- read.csv("communities.csv")

```

## Assignment 3.1
We are given some crime data. After excluding the response variable ViolentCrimesPerPop we analyse it using PCA, and find that the first component (PC1) explains about $25.017$ % of the variance, and the second component (PC2) explains about $16.936$ % of the variance in the data. To achieve $95$ % variance explained, the first 35 principal components must be used.

```{r, echo=F}
# Assignment 3.1

# Establish a scaler for the entire dataset
scaler_crime1 <- crime_df %>%
  select(-"ViolentCrimesPerPop") %>%
  preProcess()

# Rescale the entire dataset and create X_crime
X_crime <- crime_df %>%
  select(-"ViolentCrimesPerPop") %>%
  predict(scaler_crime1, .) %>%
  as.matrix()

# Compute sample covariance matrix for X_crime
S_crime <- (1/nrow(X_crime)) * t(X_crime) %*% X_crime

# Calculate eigenvalues for the sample covariance matrix
U_crime <- eigen(S_crime)

# Calculate the percentage of variance explained by each PC
var_explained <- (100*U_crime$values / sum(U_crime$values)) %>%
  round(digits = 3)

cat("Variance explained by PC1:", var_explained[1] %>% sum(), "%\n")

cat("Variance explained by PC2:", var_explained[2] %>% sum(), "%\n")

# 35 PC's needed to achieve at least 95% variance explained
cat("Variance explained by first 35 PC's:",
    var_explained[1:35] %>% sum(), "%\n")

```

## Assignment 3.2
Now we repeat the PCA performed in Assignment 3.1 using prebuilt R functionality. We find that the contributions to PC1 are fairly well spread across the variables, and the five with largest (absolute) contribution are very close in their contribution. There is clearly underlying dependencies among these variables. Median family income and median income are of course highly correlated. The other variables are also likely highly explanatory for income and wealth in general.

```{r, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.2

PCA_2 <- princomp(crime_df %>% select(-"ViolentCrimesPerPop") %>% scale())

# U contains the PC loadings
# The eigenvectors basically (columnwise)
U <- PCA_2$loadings

# Plot a trace plot for
plot(U[,1], main="Fig 3.2. Traceplot, PC1")

largest_contributors <- U[,1] %>% abs() %>% sort(decreasing = T) %>% names()
cat("Five largest contributors to PC1 are:\n")
cat(largest_contributors[1:5], sep = "\n")

```

Next, in Figure 3.3 we plot the data expressed by the vectors PC1 and PC2, and map the color of points to the Violent Crimes per Population feature. The result indicates first of all that most observations have a low value for the crime variable. There is a very large high-density region around the point $(0,0)$ (by design). This cluster extends roughly such that PC1 and PC2 seem to be negatively correlated. For low values of PC2 (roughly below -5), there is however a less dense and moderatly sized group of datapoints where the average crime variable tends to have larger values. However these datapoints with higher crime rates appear to be fairly uniformly spread across the range of PC2. The component PC1 appears to be suboptimal in predicting density, but a significantly large number of datapoints with high crime rates have negative PC1 values, compared to among the datapoints with positive PC1 values. Thus PC1 may be useful for predicting crime rate.

```{r Figure 3.3, echo=F, out.width="70%", fig.align='center'}
# Assignment 3.2

# Get the datapoints coordinates expressed in PC1 and PC2
Z_crime_2 <- X_crime %*% U_crime$vectors[,c(1,2)] %>%
  as.data.frame() %>%
  cbind(ViolentCrimesPerPop = crime_df$ViolentCrimesPerPop)

# Plot data in terms of PC1 and PC2
ggplot(data = Z_crime_2,
       aes(x = Z_crime_2[,1],
           y = Z_crime_2[,2],
           col = ViolentCrimesPerPop)) +
  scale_color_gradient(low = "lightyellow", high = "black") +
  geom_point() +
  ggtitle("Fig 3.3. Crime data expressed in PC1 and PC2") +
  xlab("PC1 value") +
  ylab("PC2 value")

```

## Assignment 3.3
Now we wish to attempt to construct a linear prediction model. First we split the available data into train and test groups (50/50). The Mean Squared Error for train and test data is reported below. There is a difference between them, but it should be noted that the MSE highly depends on problem context, and so it is difficult to make any strong statements about the significance of the difference between the achieved train and test errors. However, the test error is roughly $50$ % larger than the train error, and this might indicate overfitting.

```{r, echo=F}
# Assignment 3.3

## Split data into train and test
n = nrow(crime_df)
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=crime_df[id,]
test=crime_df[-id,]


# Establish Scaling
scaling <- train %>% preProcess()

# Scale train and test data
train <- predict(scaling, train)
test <- predict(scaling, test)

# Train lin reg model
model1 <- lm(data = train,
             formula = ViolentCrimesPerPop ~ .)

# Compute train error (MSE)
train_mse <- mean((train$ViolentCrimesPerPop - model1$fitted.values)^2)

# Predict new responses for the test data
test_pred <- predict(model1,
                     test[,-which(names(test) == "ViolentCrimesPerPop")])
test_mse <- mean((test$ViolentCrimesPerPop - test_pred)^2)

cat("Train mse:", train_mse, "\n")
cat("Test mse:", test_mse, "\n")

```

## Assignment 3.4
Now we will instead use prebuilt R functionality to try to optimise our model. We run the `optim` function and plot the resulting train and test (validation) errors against the iteration index in Figure 3.4. The optimisation quickly achieves a roughly optimal solution, and spends many iterations making small, unnecessary adjustments. We find an appropriate stopping place where the test (validation) error is minimal. When comparing the train and test errors at this point with the train and test error achieved in Assignment 3.3, we find that the train error has increased slightly (performing worse), while the test error has remained roughly the same (in fact it has decreased) so that accuracy is maintained. This indicates that the level of overfitting is lower in the new model, which is a generally desireable outcome.

It should be noted that the test data has been inappropriately used in Assignment 3.3 and 3.4, as per the explicit lab instructions. It has essentially been used as validation data. Instead, test data should be reserved and used only in the end to evaluate our final model.

```{r, echo=F}
## Assignment 3.4

# Set up data environment
train_noresponse <- train[,-which(names(train) == "ViolentCrimesPerPop")]
y_true_train <- train[,which(names(train) == "ViolentCrimesPerPop")]

test_noresponse <- test[,-which(names(test) == "ViolentCrimesPerPop")]
y_true_test <- test[,which(names(test) == "ViolentCrimesPerPop")]

train_errors <- c()
test_errors <- c()

# cost function
# data should NOT contain the response variable
# No intercept is included in the underlying model
cost_linreg <- function(theta){
  
  # Calculate train error
  y_pred_train <- as.matrix(train_noresponse) %*% theta
  train_cost <- mean((y_pred_train - y_true_train)^2)
  train_errors <<- c(train_errors, train_cost)
  
  # Calculate test error
  y_pred_test <- as.matrix(test_noresponse) %*% theta
  test_cost <- mean((y_pred_test - y_true_test)^2)
  test_errors <<- c(test_errors, test_cost)
  
  return(train_cost)
}

# Attempt to optimise our model
optim_object <- optim(rep(0, 100), method="BFGS", fn = cost_linreg, control = list(trace=T))

# Plot train and test (validation) errors
ylim <- c(min(train_errors[-c(1:500)], test_errors[-c(1:500)]), max(train_errors[-c(1:500)], test_errors[-c(1:500)]))
plot(test_errors[-c(1:500)], type="l", col="red", ylim = ylim,
     main = "Fig 3.4. Train and validation errors across iterations",
     xlab = "Iteration index",
     ylab = "Test (validation) errors")
points(train_errors[-c(1:500)], type="l", col="blue")

# Find the iteration number where the validation error is the lowest
optimal_iteration <- which(test_errors[-c(1:500)] == min(test_errors[-c(1:500)]))

# Print the resulting and train and test (validation) errors
cat("Train mse:", train_errors[optimal_iteration], "\n")
cat("Test mse:", test_errors[optimal_iteration], "\n")

```


